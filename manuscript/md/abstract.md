An embodied view of language holds that, to understand a word, you must simulate associated sensory input (e.g. simulate perception of brightness to understand 'lamp'), and prepare associated actions (e.g. prepare finger movements to understand 'typing'). To test this, we measured pupillary responses to single words that conveyed a sense of brightness (e.g. 'day') or darkness (e.g. 'night'), or were luminance-neutral (e.g. 'house'). Crucially, we found that the pupil was largest for darkness-conveying words, intermediate for neutral words, and smallest for brightness-conveying words; this pattern was found for both visually presented and spoken words, suggesting that it was due to word meaning, rather than to visual or auditory properties of the stimuli. Our findings suggest that word meaning is sufficient to trigger movement, even when this movement is not imposed by the experimental task, and even when this movement is largely beyond voluntary control.
