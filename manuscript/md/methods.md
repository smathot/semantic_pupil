## Materials and availability

Participant data, experimental scripts, and analysis scripts are available from <https://github.com/smathot/semantic_pupil>.


## Stimuli

++For the main experiments, in which we varied the semantic brightness of words, we manually selected 121 words from Lexique [@New2004], a large database with lexical properties of French words.++ There were four word categories: brightness-conveying words (e.g. *illuminé* or 'illuminated'; *N*=33), darkness-conveying words (e.g. *foncé* or 'dark'; *N*=33), neutral words (e.g. *renforcer* or 're-inforce'; *N*=35), and animal names (e.g. *lapin* or 'rabbit'; *N*=20). During the visual experiment, words were shown as dark letters (8.5 cd/m^2^) against a gray background (17.4 cd/m^2^). For the auditory experiment, words were generated by a synthetic voice (using the Mac OS X 'say' TTS).

Because we wanted to compare pupillary responses to brightness- and darkness-conveying words, these two categories needed to be matched as accurately as possible. We focused on two main properties: lexical frequency, how often a word occurs in books (bright: M=41 per million, SD=147; dark: M=39, SD=119); and, for the visual experiment, visual intensity (bright: M=1.58×10^6^ arbitrary units, SD=4.31×10^5^; dark: M=1.56×10^6^, SD=4.26×10^5^). Visual intensity was matched by selecting words that had approximately the same number of letters, then generating images of these words, and finally iteratively resizing these images until the visual intensity (i.e. summed luminance) of the words was almost identical between the two categories.

In the end, we had a stimulus set in which darkness- and brightness-conveying words were very closely matched; however, as a result of our stringent criteria, our set contained several variations of the same words, such as *briller* ('to shine') and *brillant* ('shining'). But given the pupil's sensitivity to slight differences in task difficulty (i.e. lexical frequency) and visual intensity, we felt that matching was more important than having a highly varied stimulus set.

++For the control experiment, in which we varied the valence of words, we selected 60 words from @Bonin2003, complemented with the 20 animal names selected for the main experiments. Positive (e.g. *cadeau* or 'present'; *N*=30; valence > 3.5 on a 1 to 5 scale) and negative words (e.g. *cicatrice* or 'scar'; *N*=30; valence < 2.5) were matched on lexical frequency (positive: M=3.21, SD=0.76; negative: M=3.26, SD=0.53) and visual intensity (positive: M=1.15×10^6^, SD=3.47×10^5^; negative: M=1.15×10^6^, SD=3.48×10^5^), and did not have any obvious association with brightness or darkness.++

++For all experiments, stimuli were manually selected based on strict criteria. Our sample size of around 30 words per condition was therefore a compromise between having well-matched stimuli, and a reasonable number of observations per participant and condition.++


## Pupillometry experiments

Thirty naive observers (age range: 18-54 y; 21 women) participated in the visual experiment. ++Thirty other naive observers participated in the auditory experiment (age range: 18-31 y; 19 women). And thirty naive observers participated in the control experiment, four of whom had participated also in the auditory experiment (age range: 18-31 y; 19 women). We used two to three times as many participants per experiment as in previous studies on the pupillary light response [e.g. *N*=5-15 in @Binda2013Bright;@Mathôt2013Plos, but see *N*=52 in Exp 5 from @Laeng2014Imaginary], because we expected the effect of embodied language on pupil size to be relatively small.++ Participants reported normal or corrected vision, provided written informed consent prior to the experiment, and received €6 for their participation. The experiment was conducted with approval of the *Comité d'éthique de l'Université d'Aix-Marseille* (Ref.: 2014-12-03-09).

Pupil size was recorded monocularly with an EyeLink 1000 (SR Research, Missisauga, Ontario, Canada), a video-based eye tracker sampling at 1000 Hz. Stimuli were presented on a 21" ViewSonic p227f CRT monitor (1024 x 768 px, 150 Hz). Testing took place in a dimly lit room. The experiment was implemented with OpenSesame [@MathôtSchreijTheeuwes2012] using the Expyriment [@KrauseLindemann2014] backend.

At the beginning of each session, a nine-point eye-tracker calibration was performed. Before each trial, a single-point recalibration (drift correction) was performed. Each trial started with a dark central fixation dot on a gray background for 3 s. Next, a word was presented. In the visual experiment and the control experiment, the word was presented visually in the center of the screen for 3 s, or until the participant pressed the space bar; in the auditory experiment, the word was played back through a set of desktop speakers, and the experiment paused for 3 s, or until the participant pressed the space bar. The participant's task was to press the space bar whenever they saw or heard an animal name, and to withhold response otherwise. Participants saw or heard each word once, with the exception of *pénombre* in the visual experiment, which, due to a bug in the experiment, was shown twice. (This is why there are slightly more darkness- than brightness-conveying trials, as shown in %FigMainResults.) Word order was fully randomized.


## Normative ratings

++For all brightness- and darkness-conveying words++, we collected normative ratings from thirty naive observers (age range: 18-29 y; 17 women), most of whom had not participated in the pupillometry experiments. Participants received €2 for their participation.

Words were presented, one at a time and using the same images as used for the visual pupillometry experiment, together with a five-point rating scale. On this scale, participants indicated how strongly the word conveyed a sense of brightness ('very bright' to 'very dark'), or, in a different phase of the experiment, the word's valence ('very negative' to 'very positive'). Brightness and valence were rated in separate blocks, the order of which was counterbalanced across participants. Based on valence ratings, we calculated the emotional intensity of the words, as the deviation from neutral valence (intensity = |3-valence|).
